{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c862f1-59a5-40bd-9932-125fd659f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required Packages and Modules \n",
    "# !pip install opencv-python\n",
    "# !pip install matplotlib\n",
    "# !pip install tensorflow\n",
    "# !pip install scikit-learn\n",
    "\n",
    "#Uncomment the above pip commands to install the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b1b7d-e92b-4ad5-a700-ea8592695a83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.callbacks import Callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1caabab8-89ee-41e6-b4b3-da673c414d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a path object \n",
    "data_dir = Path(\"./Dataset/BHSig260-Hindi\") #Change the path as needed\n",
    "\n",
    "# Labeling the images\n",
    "images, labels = [],[]\n",
    "\n",
    "for dir in data_dir.iterdir():\n",
    "    if dir.is_dir():\n",
    "        for i in dir.glob('*.tif'):\n",
    "            label = 0 if 'G' in i.stem else 1\n",
    "            images.append(i)\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c24dd5-2d11-48a3-93bf-c74ddb887a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for preprocessing each image\n",
    "def load_and_preprocess_image(path, label):\n",
    "    def _load_image(path, label):\n",
    "        path = path.numpy().decode('utf-8')\n",
    "        #Resizing the image to the required image size for mobilenet(224x224)\n",
    "        image = tf.keras.preprocessing.image.load_img(path, target_size=(224, 224))\n",
    "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        #Preprocessing \n",
    "        image = preprocess_input(image)\n",
    "        return image, label\n",
    "\n",
    "    [image, label] = tf.py_function(_load_image, [path, label], [tf.float32, tf.int32])\n",
    "    image.set_shape((224, 224, 3))\n",
    "    label.set_shape([])  # Ensure label is a scalar\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f75b7c38-90ed-41a7-a1f6-96a71082a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Assuming 'images' is a list of image file paths (as strings) and 'labels' are the corresponding labels\n",
    "# First, ensure your 'images' list contains string paths, not PosixPath objects\n",
    "images = [str(img_path) for img_path in images]  # Convert PosixPath to string if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202ddcc5-5e33-41a7-b689-9234e2f1b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and temp sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, train_size=0.7, stratify=labels)\n",
    "\n",
    "# Split the temp set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd27465f-1611-4034-8cc9-a67f23dc884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion to tensorflow dataset, Preprocess each image and shuffle the training dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds = train_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(buffer_size=1000).batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18bbf423-4647-4ebf-bca1-86c5d8faf76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion to tensorflow dataset, Preprocess each image and shuffle the validation dataset\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_ds = val_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.shuffle(buffer_size=1000).batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e27ae8df-7015-4bb0-88b8-9ce75fcdca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion to tensorflow dataset, Preprocess each image and shuffle the test dataset\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_ds = test_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_ds = test_ds.batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af139a98-fa0c-498c-aaac-13f19e2439a9",
   "metadata": {},
   "source": [
    "The Preprocessing stage ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce2c769-0cff-459c-93f0-0479eea28510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 41 41\n",
      "Image shape: (None, 224, 224, 3) Label: Tensor(\"args_1:0\", shape=(None,), dtype=int32)\n",
      "Image shape: (None, 224, 224, 3) Label: Tensor(\"args_1:0\", shape=(None,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds), len(test_ds))\n",
    "\n",
    "def check_shapes(image, label):\n",
    "    print(\"Image shape:\", image.shape, \"Label:\", label)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(check_shapes)\n",
    "val_ds = val_ds.map(check_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92e00b07-621a-4281-bb93-358b3f19307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 931ms/step - accuracy: 0.6753 - auc: 0.7275 - f1_score: 0.7095 - loss: 0.7427 - precision_1: 0.7024 - recall_1: 0.7180 - val_accuracy: 0.6397 - val_auc: 0.7267 - val_f1_score: 0.7296 - val_loss: 0.7101 - val_precision_1: 0.6256 - val_recall_1: 0.8750\n",
      "Epoch 2/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 886ms/step - accuracy: 0.8616 - auc: 0.9319 - f1_score: 0.8755 - loss: 0.3307 - precision_1: 0.8709 - recall_1: 0.8803 - val_accuracy: 0.6713 - val_auc: 0.7372 - val_f1_score: 0.7351 - val_loss: 0.6372 - val_precision_1: 0.6655 - val_recall_1: 0.8208\n",
      "Epoch 3/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 887ms/step - accuracy: 0.9259 - auc: 0.9772 - f1_score: 0.9331 - loss: 0.2028 - precision_1: 0.9263 - recall_1: 0.9401 - val_accuracy: 0.6721 - val_auc: 0.7708 - val_f1_score: 0.7475 - val_loss: 0.6532 - val_precision_1: 0.6532 - val_recall_1: 0.8736\n",
      "Epoch 4/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 893ms/step - accuracy: 0.9683 - auc: 0.9950 - f1_score: 0.9714 - loss: 0.1128 - precision_1: 0.9650 - recall_1: 0.9780 - val_accuracy: 0.7037 - val_auc: 0.7927 - val_f1_score: 0.7684 - val_loss: 0.6302 - val_precision_1: 0.6791 - val_recall_1: 0.8847\n",
      "Epoch 5/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 872ms/step - accuracy: 0.9844 - auc: 0.9985 - f1_score: 0.9857 - loss: 0.0730 - precision_1: 0.9836 - recall_1: 0.9879 - val_accuracy: 0.6952 - val_auc: 0.8030 - val_f1_score: 0.7713 - val_loss: 0.6860 - val_precision_1: 0.6614 - val_recall_1: 0.9250\n",
      "Epoch 6/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 907ms/step - accuracy: 0.9957 - auc: 0.9999 - f1_score: 0.9961 - loss: 0.0365 - precision_1: 0.9980 - recall_1: 0.9941 - val_accuracy: 0.6952 - val_auc: 0.8071 - val_f1_score: 0.7723 - val_loss: 0.7039 - val_precision_1: 0.6601 - val_recall_1: 0.9306\n",
      "Epoch 7/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 873ms/step - accuracy: 0.9984 - auc: 1.0000 - f1_score: 0.9985 - loss: 0.0208 - precision_1: 0.9994 - recall_1: 0.9978 - val_accuracy: 0.7014 - val_auc: 0.8182 - val_f1_score: 0.7759 - val_loss: 0.7048 - val_precision_1: 0.6653 - val_recall_1: 0.9306\n",
      "Epoch 8/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 934ms/step - accuracy: 0.9990 - auc: 1.0000 - f1_score: 0.9991 - loss: 0.0131 - precision_1: 0.9993 - recall_1: 0.9988 - val_accuracy: 0.7284 - val_auc: 0.8254 - val_f1_score: 0.7874 - val_loss: 0.6418 - val_precision_1: 0.6966 - val_recall_1: 0.9056\n",
      "Epoch 9/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 945ms/step - accuracy: 0.9995 - auc: 1.0000 - f1_score: 0.9996 - loss: 0.0096 - precision_1: 0.9991 - recall_1: 1.0000 - val_accuracy: 0.7160 - val_auc: 0.8152 - val_f1_score: 0.7812 - val_loss: 0.7034 - val_precision_1: 0.6830 - val_recall_1: 0.9125\n",
      "Epoch 10/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 873ms/step - accuracy: 0.9997 - auc: 1.0000 - f1_score: 0.9998 - loss: 0.0063 - precision_1: 0.9997 - recall_1: 0.9998 - val_accuracy: 0.7106 - val_auc: 0.8183 - val_f1_score: 0.7769 - val_loss: 0.7163 - val_precision_1: 0.6795 - val_recall_1: 0.9069\n",
      "Epoch 11/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 871ms/step - accuracy: 0.9994 - auc: 1.0000 - f1_score: 0.9995 - loss: 0.0061 - precision_1: 0.9992 - recall_1: 0.9997 - val_accuracy: 0.7469 - val_auc: 0.8374 - val_f1_score: 0.7892 - val_loss: 0.5724 - val_precision_1: 0.7344 - val_recall_1: 0.8528\n",
      "Epoch 12/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 876ms/step - accuracy: 0.9999 - auc: 1.0000 - f1_score: 0.9999 - loss: 0.0043 - precision_1: 0.9998 - recall_1: 1.0000 - val_accuracy: 0.7091 - val_auc: 0.8373 - val_f1_score: 0.7812 - val_loss: 0.7695 - val_precision_1: 0.6710 - val_recall_1: 0.9347\n",
      "Epoch 13/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 910ms/step - accuracy: 0.9999 - auc: 1.0000 - f1_score: 0.9999 - loss: 0.0039 - precision_1: 1.0000 - recall_1: 0.9998 - val_accuracy: 0.7037 - val_auc: 0.8348 - val_f1_score: 0.7788 - val_loss: 0.8403 - val_precision_1: 0.6654 - val_recall_1: 0.9389\n",
      "Epoch 14/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 889ms/step - accuracy: 1.0000 - auc: 1.0000 - f1_score: 1.0000 - loss: 0.0031 - precision_1: 1.0000 - recall_1: 1.0000 - val_accuracy: 0.7292 - val_auc: 0.8325 - val_f1_score: 0.7877 - val_loss: 0.7090 - val_precision_1: 0.6977 - val_recall_1: 0.9042\n",
      "Epoch 15/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 875ms/step - accuracy: 1.0000 - auc: 1.0000 - f1_score: 1.0000 - loss: 0.0020 - precision_1: 1.0000 - recall_1: 1.0000 - val_accuracy: 0.7361 - val_auc: 0.8341 - val_f1_score: 0.7899 - val_loss: 0.7083 - val_precision_1: 0.7081 - val_recall_1: 0.8931\n",
      "Epoch 16/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 885ms/step - accuracy: 1.0000 - auc: 1.0000 - f1_score: 1.0000 - loss: 0.0018 - precision_1: 1.0000 - recall_1: 1.0000 - val_accuracy: 0.7485 - val_auc: 0.8406 - val_f1_score: 0.7973 - val_loss: 0.6841 - val_precision_1: 0.7218 - val_recall_1: 0.8903\n",
      "Epoch 17/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 930ms/step - accuracy: 1.0000 - auc: 1.0000 - f1_score: 1.0000 - loss: 0.0015 - precision_1: 1.0000 - recall_1: 1.0000 - val_accuracy: 0.7539 - val_auc: 0.8424 - val_f1_score: 0.7977 - val_loss: 0.6736 - val_precision_1: 0.7340 - val_recall_1: 0.8736\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 11:17:29.157453: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 940 of 1000\n",
      "2024-04-22 11:17:29.613299: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 877ms/step - accuracy: 0.9999 - auc: 1.0000 - f1_score: 0.9999 - loss: 0.0016 - precision_1: 0.9998 - recall_1: 1.0000 - val_accuracy: 0.7670 - val_auc: 0.8372 - val_f1_score: 0.7855 - val_loss: 0.6135 - val_precision_1: 0.8038 - val_recall_1: 0.7681\n",
      "Epoch 19/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 928ms/step - accuracy: 0.9996 - auc: 1.0000 - f1_score: 0.9996 - loss: 0.0027 - precision_1: 1.0000 - recall_1: 0.9992 - val_accuracy: 0.7832 - val_auc: 0.8606 - val_f1_score: 0.7892 - val_loss: 0.6169 - val_precision_1: 0.8581 - val_recall_1: 0.7306\n",
      "Epoch 20/20\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 930ms/step - accuracy: 1.0000 - auc: 1.0000 - f1_score: 1.0000 - loss: 0.0012 - precision_1: 1.0000 - recall_1: 1.0000 - val_accuracy: 0.7693 - val_auc: 0.8662 - val_f1_score: 0.7662 - val_loss: 0.6824 - val_precision_1: 0.8766 - val_recall_1: 0.6806\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the MobileNetV3Small model, excluding the top fully connected layer\n",
    "base_model = MobileNetV3Small(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Set all layers of the base model to be trainable\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Adding custom layers\n",
    "x = base_model.output\n",
    "x = Flatten()(x)  # Flatten the output\n",
    "predictions = Dense(1, activation='sigmoid')(x)  # Final dense layer for binary classification\n",
    "\n",
    "# Creating the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.round(y_pred)\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "\n",
    "f1_metric = F1Score()\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), f1_metric, AUC()])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_ds, epochs=20, validation_data=val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1a33cc2-a198-4d87-902a-f6064033228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"MobileNetV3Small_without_augmentation.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94ae31cf-e81b-4d4c-bee6-757e38bad0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 215ms/step - accuracy: 0.7395 - auc: 0.8590 - f1_score: 0.7308 - loss: 0.6726 - precision_1: 0.8509 - recall_1: 0.6406\n",
      "Test Loss: 0.727\n",
      "compile_metrics: 0.735\n",
      "[0.7273659110069275, 0.7353395223617554, 0.8563327193260193, 0.6291666626930237, 0.7253802418708801, 0.854687511920929]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Custom F1-Score Metric\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.round(y_pred)\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "\n",
    "model = load_model('MobileNetV3Small_without_augmentation.keras', custom_objects={'F1Score': F1Score})\n",
    "\n",
    "evaluation_results = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {evaluation_results[0]:.3f}\")\n",
    "\n",
    "# Since the first value is always the loss, you can print out the rest as metrics\n",
    "for i, metric in enumerate(model.metrics_names[1:], 1):\n",
    "    print(f\"{metric}: {evaluation_results[i]:.3f}\")\n",
    "print(evaluation_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
